---
title: "忘却曲線アプリを作ったら Anki の再発明だった話 — そして PDF→Anki 自動生成 CLI を作るまで"
emoji: "🃏"
type: "tech"
topics: ["Claude", "Anki", "Python", "CLI", "LLM"]
published: false
---

## 忘却曲線アプリを作ったら Anki の再発明だった

G検定の勉強をしていた。400問以上の問題を効率よく復習したくて、忘却曲線に基づいた出題アプリを作り始めた。

最初は Python/Streamlit で Web 版（g-kentei-app）。SM-2 アルゴリズムで復習タイミングを計算し、Google Sheets にデータを同期する。159 テスト、カバレッジ 96%。1週間で動くものができた。

しかし通勤中にオフラインで使えない。Streamlit の Web UI はモバイルで微妙。ならばネイティブアプリだ、と Swift/SwiftUI で iOS 版（g-kentei-ios）を作り直した。途中で SM-2 を FSRS（精度 47.1% → 89.6%）に移行。iCloud Drive でオフライン同期対応。566 テスト。7日で完成。

### 410 問のデータが壊れた日

アプリ本体より辛かったのは、問題データの作成だった。

975KB のテキストファイルから 410 問を正規表現パーサーで抽出した。動いた、と思った。しかしテストしてみると、第 2 章の問題に第 1 章の解説が紐付いている。調べると、ソースファイルの問題セクションは「第 1 章」（スペースあり）、解答セクションは「第1章」（スペースなし）と表記が揺れていた。正規表現 `第(\d+)章` がスペースなし版しかマッチしない。

**`\s*` が 1 つ足りなかっただけで、410 問の問答対応が全壊した。**

パニックになって「LLM でパースすれば柔軟に対応できるのでは」と 388 行の移行計画を書いた。instructor + pydantic + OpenAI で構造化パース。しかし冷静になって考えると、オフライン動作のために Streamlit を捨てて Swift に移行したのに、テキストのパースで API に依存するのは本末転倒だ。

結局、正規表現を修正して `\s*` を入れただけで 98% の精度が出た。残り 2% の低信頼度データだけ LLM に回す信頼度スコアリングを追加。410 問中 402 問は正規表現だけで処理できた。LLM のコストは $0.10 以下。

**学び: LLM に飛びつく前に、まず壊れている正規表現を直せ。**

### 気づき: これは Anki の再発明だ

機能が揃ってきたところで気づいた。**自分が作っているのは Anki そのものだ。**

忘却曲線に基づくスケジューリング、カードの表裏表示、進捗管理。全部 Anki にある。しかも Anki は 20 年以上の歴史があり、プラグインエコシステムも充実している。2 週間で作ったものが、Anki の車輪の再発明でしかなかった。

では本当に足りないものは何だったのか。**コンテンツだ。**

410 問の問題データで一番苦労したのは、アプリではなくパーサーだった。テキストからカードを作る作業が退屈で、ミスが入り、スケールしない。アプリ側は Anki に任せて、「どんなテキストからでも高品質な Anki カードを自動生成する」ツールを作るべきだった。

それが pdf2anki の始まりだ。

## pdf2anki: Claude API で PDF→Anki カードを作る

pdf2anki は CLI ツールだ。PDF、テキスト、Markdown を入力すると、Claude API を使って Anki にインポートできるフラッシュカード（TSV/JSON）を出力する。

```bash
$ pdf2anki convert textbook.pdf -o cards.tsv
```

入力テキストを分析し、8 種類のカード（QA、穴埋め、用語定義、比較対照など）を Bloom のタキソノミーに基づいて生成する。依存は 8 個だけ。

ここからは、作る中で躓いたことを書く。

## LLM の出力は信用できない

LLM にカード生成を任せると、まず JSON が壊れる。マークダウンのコードブロックで囲んでくることもあれば、余計なテキストが混じることもある。パースに成功しても、中身の品質はバラバラだ。

**問題の具体例:**
- 表面が「説明してください」だけの曖昧なカード
- 裏面が 500 文字を超える長文回答
- 1 枚のカードに 3 つの概念が詰め込まれている
- リスト問題なのに QA 形式で出力（穴埋めにすべき）

「LLM が生成したから OK」では使えない。品質保証の仕組みが必要だった。

### 3 層の品質パイプライン

最終的に 3 層構成にした。

**第 1 層: ヒューリスティックスコアリング**

コードで判定できるものは LLM を呼ばずに判定する。6 つの軸で 0〜1 のスコアを付け、重み付き合計が 0.90 以上なら合格。

| 軸 | 重み | チェック内容 |
|---|---|---|
| 表面の質 | 25% | 10〜200 文字、疑問符の有無 |
| 裏面の質 | 25% | 5〜200 文字、簡潔さ |
| カード種別の適合 | 15% | リスト内容なのに QA になっていないか |
| Bloom レベル | 10% | 認知レベルが内容に合っているか |
| タグの質 | 10% | 階層タグが付いているか |
| 原子性 | 15% | 1 枚 1 概念になっているか |

原子性の判定が面白い。裏面の文を句点で分割し、文数でスコアを落とす。「また、」「さらに」「加えて」といった接続詞を検出したらさらに減点。

```python
# 裏面が3文以上 → 複数概念の疑い
sentences = [s for s in _SENTENCE_SPLIT_RE.split(back) if s.strip()]
if len(sentences) >= 3:
    score = max(0.3, 1.0 - len(sentences) * 0.15)

# 「また、」「さらに」→ 追加減点
if _MULTI_CONCEPT_RE.search(back):
    score = max(0.2, score - 0.15)
```

**第 2 層: LLM による批評**

ヒューリスティックで 0.90 未満のカードだけを Claude に再評価させる。LLM の批評結果は 3 択: `improve`（書き直し）、`split`（分割）、`remove`（削除）。最大 2 ラウンド。

ポイントは、**全カードに LLM を使わない**こと。0.90 以上のカードは第 1 層で素通りさせる。これでコストと品質のバランスを取っている。

**第 3 層: 重複検出**

文字バイグラムの Jaccard 類似度で重複を検出する。表面の類似度が 0.7 を超えたら重複フラグ。

### JSON パースの防御

LLM の出力が壊れたとき、全体を落とすのではなく、カード単位でスキップする。

```python
for i, item in enumerate(data):
    try:
        card = AnkiCard.model_validate(item)
        cards.append(card)
    except (ValidationError, TypeError) as e:
        logger.warning("Skipping invalid card at index %d: %s", i, e)
```

10 枚中 1 枚が壊れていても、残り 9 枚は救う。この「部分的な成功を許容する」設計は、LLM を使うツール全般に言えることだと思う。

## コストが見えないと怖い

Claude API の料金は従量制だ。Sonnet で入力 $3/M tokens、出力 $15/M tokens。100 ページの PDF を処理したらいくらかかるか、実行前にわからないのは怖い。

### 事前コスト見積もり

`preview` コマンドで、実際に API を叩かずにコスト見積もりを出せるようにした。

```bash
$ pdf2anki preview textbook.pdf
Estimated cost: $0.42 (Sonnet) / $0.11 (Haiku)
Sections: 12 | Chunks: 8 | Tokens: ~45,000
```

### budget_limit で暴走防止

CostTracker に `budget_limit`（デフォルト $1.00）を設定し、API コールのたびに累積コストをチェックする。超えたらそこで止まる。

```python
@dataclass(frozen=True, slots=True)
class CostTracker:
    budget_limit: float = 1.00
    records: tuple[CostRecord, ...] = ()
```

frozen dataclass で不変にしている。API コールのたびに新しい CostTracker を返すパターンだ。g-kentei-ios でのイミュータブルモデル設計がここで活きた。

### 自動モデル選択

テキスト量が少ない（10,000 文字未満、30 枚未満）ならコストが 3.75 倍安い Haiku を使い、多ければ Sonnet にルーティングする。

```python
_SONNET_TEXT_THRESHOLD = 10_000   # 文字数
_SONNET_CARD_THRESHOLD = 30      # カード枚数
```

単純だが効果は大きい。短いテキストの処理コストが大幅に下がる。

## 長文 PDF をどう分割するか

Claude の入力トークン上限は約 200K だが、一度に大量のテキストを投げるとカードの品質が落ちる。15 万トークン（約 60 万文字）をデフォルト上限にして、チャンク分割する設計にした。

### 最初の失敗: 機械的な分割

最初はテキストを文字数で均等に切っていた。当然、章の途中で切れる。「第 3 章の後半」と「第 4 章の前半」が混ざったチャンクから生成されるカードは、文脈がおかしい。

### セクション分割 + breadcrumb

Markdown の見出し（`#`、`##`、`###`）で論理的に分割する方式に変えた。各セクションに「どこの章のどの節か」を示す breadcrumb を付与する。

```
breadcrumb: "本論 > 第1章 論書名の意味 > 1.1 語源"
```

heading-stack という辞書で「今どの階層にいるか」を追跡する。H2 が出現したら、それ以下の H3 情報をクリアする。

```python
heading_stack: dict[int, str] = {}
# H2 出現 → H3 をクリア
keys_to_remove = [k for k in heading_stack if k >= level]
```

1 セクションが 30,000 文字を超えたら段落単位でサブ分割する。見出しに「(cont.)」を付けて連続セクションだとわかるようにした。

### 日本語テキストの罠

Markdown 見出しがないテキストもある。日本語の書籍なら「第一章」「（1）」「一、」といったパターンで見出しを検出するフォールバックを入れた。

もう一つの罠は、トークン推定。`CHARS_PER_TOKEN = 4` という定数を使っているが、これは英語の値だ。日本語は 1 トークンあたり 2〜3 文字。つまり日本語テキストのコスト推定は実際の半分程度になる。これは今も未修正の既知の問題。

## OpenAI 対応を捨てた話

「Claude だけでなく OpenAI も使えるようにすべきでは？」と考えた。OpenAI の $10 クレジットが残っていたからだ。

実装コストを見積もった。Provider プロトコルを定義し、Anthropic と OpenAI の実装を作り、既存テストをプロバイダーレベルのモックに移行する。見積もり: 1,000 行、10 時間。

しかし問題は、テストだった。既存テストには **209 箇所の mock/patch** が Anthropic の内部 API（`_call_claude_api`、`anthropic.Anthropic()`）を直接モックしていた。プロバイダー抽象化への移行は find-replace では済まない。1 つ 1 つ書き換える必要がある。

ROI を計算した。

| 投資 | リターン |
|---|---|
| 1,000 行、10 時間 | $10 クレジット消費 |
| **時給換算: $1/h** | |

$1/時。最低賃金以下だ。

その 10 時間で何ができるか比較した。

- 画像カード生成（Vision API 統合）: 8 時間、**新しい能力の獲得**
- Interactive Review TUI: 6 時間、**UX の大幅改善**
- Eval Framework: 4 時間、**品質計測基盤**

どれも OpenAI 対応より価値が高い。OpenAI 対応はスキップして、この 3 つを実装した。

**学び:** 機能追加の前に「これをやらない場合、同じ時間で何ができるか」を考える。$10 を回収するために 1,000 行書くのは、合理的に見えて合理的ではない。

## 画像カードという沼

PDF にはテキストだけでなく図やチャートもある。これをカードに活かしたい。Claude の Vision API を使えば画像を理解してカードを生成できる。

g-kentei-ios でも同じ問題があった。教科書の図表が問題に絡むのに、テキスト抽出では図が消える。あのときは諦めた。pdf2anki では正面から取り組むことにした。

### 「全ページ画像化」の誘惑

最初に思いついたのは、全ページを画像として Vision API に投げるアプローチだ。テキスト抽出のミスも防げるし、レイアウトも保持できる。理想的に見える。

しかしコストを計算して目が覚めた。画像のトークンコストは `(幅 × 高さ) ÷ 750`。100 ページの PDF を全ページ画像化すると、画像だけで 15 万トークン。テキストと合わせて 30 万トークン、$1 近くかかる。テキストだけなら $0.15 程度。**7 倍のコスト差**。

### 3 つの妥協点

結局、3 つの制約を設けた。

**1. ページ単位の画像カバレッジ閾値: 20%**

pymupdf でページ内の画像面積を計算し、ページ面積の 20% 以上を画像が占めるページだけ Vision API に回す。テキスト主体のページはテキスト抽出で十分。

**2. DPI: 150**

Claude Vision API の制約は長辺 1568px、最大 1.15 メガピクセル。PDF を何 DPI でラスタライズするかが問題になる。

- 300 DPI: きれいだがトークンが倍。1 画像 3,000 トークン超
- 72 DPI: 安いが図中の文字が潰れる
- **150 DPI**: 文字が読める最低限の解像度。リサイズ後 1 画像 1,500 トークン程度

**3. 1 ページ最大 5 画像**

図が 10 枚あるページでも 5 枚で打ち切る。5 枚 × 1,500 トークン = 7,500 トークン。budget_limit と組み合わせれば暴走しない。

これら 3 つの妥協で、Vision ありの処理コストをテキストのみの 1.5〜2 倍程度に抑えた。7 倍にはならない。

## プロンプトの品質をどう測るか

プロンプトを書き換えたとき、カードの品質が上がったのか下がったのかわからない。主観で「良くなった気がする」では心許ない。

### 期待カードとのキーワードマッチ

「このテキストからはこういうカードが生成されるべき」という期待値を YAML で定義する。

```yaml
- id: "buddhism-01"
  text: |
    四諦とは、仏教の根本教義であり、苦諦・集諦・滅諦・道諦の
    4つの真理を指す。
  expected_cards:
    - front_keywords: ["四諦", "何"]
      back_keywords: ["苦諦", "集諦", "滅諦", "道諦"]
      card_type: qa
```

生成されたカードの表面と裏面にキーワードが含まれているかで一致度を計算する。Recall（期待カードのうち何枚生成されたか）、Precision（生成カードのうち何枚が正しいか）、F1 を出力。

```bash
$ pdf2anki eval --dataset evals/dataset.yaml
Recall: 0.78 | Precision: 0.85 | F1: 0.81
```

### キーワードマッチの限界

この方式はシンプルだが限界がある。「ニューラルネットワーク」と「神経回路網」が同じ概念だと判定できない。同義語辞書やエンベディング類似度を使えば精度は上がるが、それは将来の課題にした。

重要なのは「プロンプト変更で回帰が起きていないか」を検知できることで、完璧なマッチングではない。

## まとめ: 3 つのプロジェクトで学んだこと

振り返ると、g-kentei-app → g-kentei-ios → pdf2anki は 10 日間の連続した試行錯誤だった。

**g-kentei-app** で「忘却曲線アプリが欲しい」と思い、**g-kentei-ios** で「ネイティブにすれば解決する」と信じ、最後に「**解くべき問題はアプリじゃなくてコンテンツだった**」と気づいた。

各プロジェクトで学んだことが次のプロジェクトの設計に効いている。

- g-kentei-app の **SM-2 実装** → FSRS の理解の土台になった
- g-kentei-ios の **パーサー危機**（`\s*` 1 個で 410 問全壊）→ LLM に飛びつく前に既存コードを直す判断力
- g-kentei-ios の **regex vs LLM の判断** → pdf2anki の品質パイプラインで「ヒューリスティックで済むなら LLM を呼ばない」設計
- g-kentei-ios の **immutable model** → pdf2anki の frozen dataclass パターン（CostTracker、Section、AnkiCard すべて不変）

pdf2anki 自体でも躓きの連続だった。

- LLM の出力が壊れる → **3 層品質パイプライン**で防御
- コストが見えない → **事前見積もり + budget_limit** で可視化
- 長文が切れる → **セクション分割 + breadcrumb** で文脈保持
- OpenAI 対応したい → **ROI 計算**して捨てる判断
- 画像を活かしたい → **3 つの妥協点**でコストを抑制
- 品質が測れない → **キーワードマッチ Eval** で「完璧より実用」

どれも「理想の設計」ではなく「現実との妥協」の産物だ。LLM を使ったツールを作ると、こういう泥臭い判断の連続になる。この記事が同じ道を歩く人の参考になれば幸いだ。

pdf2anki は [GitHub で公開](https://github.com/shimomoto-tatsuya/pdf2anki)している。依存 8 個、624 テスト、カバレッジ 92%。
